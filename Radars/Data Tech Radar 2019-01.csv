name,ring,quadrant,isNew,description
Jupyter,adopt,tools,TRUE,"<a href=""https://jupyter.org/"">Jupyter</a> is one of the two adopted notebook tools we use in the Data teams at Scout24. It has widespread adoption in the industry and a large, supportive community."
Zeppelin,adopt,tools,TRUE,"In addition to Jupyter, we also adopted <a href=""https://zeppelin.apache.org/"">Zeppelin</a> notebooks within some of the data teams. While Jupyter tends to be used for python-based ad hoc analysis and machine learning use cases, Zeppelin is more focused on Scala and is used heavily to prototype Spark-based data pipelines in Scala."
CloudHealth,adopt,tools,TRUE,"<a href=""https://cloudhealthtech.com/"">CloudHealth</a> is a tool for analysing our AWS costs. It is our main tool for keeping an eye on costs and investigating any anomolies. We use it instead of the built-in cost tracking features of AWS because it can aggregate costs across all our accounts."
Pandas,adopt,tools,TRUE,"<a href=""https://pandas.pydata.org/"">Pandas</a> is a Python library that provides high-level data structures and a vast variety of tools for analysis, used everywhere."
DataWario,adopt,tools,TRUE,"<a href=""https://github.com/Scout24/datawario"">DataWario</a> is a tool for analysing our AWS costs. It is our main tool for keeping an eye on costs and investigating any anomolies. We use it instead of the built-in cost tracking features of AWS because it can aggregate costs across all our accounts."
Docker,adopt,tools,TRUE,"<a href=""https://www.docker.com/"">Docker</a> is the standard for containerization of services and we follow industry standard practises by adopting docker."
PAC,adopt,tools,TRUE,"Personal Analytics Cluster (PAC) is an in-house tool for running queries on Data Lake. It utilizes Jupyter and Zeppelin notebooks running on an auto-scaling Hadoop cluster. You can write Scala Spark or pySpark jobs and you may also use Hue to run SQL-based queries using Hive. We use it to spike some use cases and ad hoc analysis."
AWS CloudFormation,adopt,tools,TRUE,"<a href=""https://aws.amazon.com/cloudformation/"">AWS CloudFormation</a> provides a common language to describe and provision all the infrastructure resources in our cloud environment. We use it to have our infrastructure as code  so we can recreate it seamlessly."
Dask,trial,tools,TRUE,"<a href=""http://docs.dask.org/en/latest/"">Dask</a> enables distributed computing without the overhead of the JVM, we are actively using it in production."
FiZZ,trial,tools,TRUE,"<a href=""https://docs.cloud.scout24.com/products/fizz/"">FiZZ</a> is a cloud CI/CD solution based on Jenkins and managed by Delivery Engineering. It allows teams to set up their own build and deploy environments without interfering with other teams, while giving them access to a (theoretically) unlimited number of build servers. Jenkins and thus FiZZ allows writing build pipelines as code, making it easy to version control and test the build pipeline together with the code being built."
PyTorch,assess,tools,TRUE,"<a href=""https://pytorch.org/"">PyTorch</a> is an interesting alternative to TensorFlow that is developed by Facebook."
JupyterLab,assess,tools,TRUE,"<a href=""https://jupyterlab.readthedocs.io"">JupyterLab</a> is the next-generation web-based user interface for Project Jupyter."
FBProphet,assess,tools,TRUE,"<a href=""https://facebook.github.io/prophet/"">FBProphet</a> is library for building Bayesian Structural time series models, developed and actively maintained by Facebook."
Grafana,hold,tools,TRUE,"<a href=""https://grafana.com/"">Grafana</a> was the standard metric graphing and dashboard tool for data center applications. While it is possible to run in AWS, it requires its own instance to run and maintain. Therefore, we put the use of Grafana on hold in favour of DataDog."
RStudio,hold,tools,TRUE,"<a href=""https://rstudio.com/"">RStudio</a> An IDE for R. Some past data science projects were developed in R, RStuidio offers a whole suite of libraries. DS team has fully adopted Python as the development language, which is easier for deployment process, hence RStudio is on hold."
TeamCity,hold,tools,TRUE,"<a href=""https://www.jetbrains.com/teamcity/"">TeamCity</a> is based on a centralized model where all engineering teams use the same installation. Scaling the services, limiting blast-radius and implementing security isolation is very difficult with this model. Although TeamCity is still used heavily within Data Technology teams, we would like to move away from it to FiZZ, Scout24's in-house solution for de-centralized Jenkins."
SQL,adopt,languages & frameworks,TRUE,"<a href=""https://en.wikipedia.org/wiki/SQL"">SQL</a> is a domain-specific language for defining, manipulating and querying data structures. It has its origins as an interface for relational databases, like Oracle and MySQL, but has since been adopted for non-relational datastores as well, for example AWS provides SQL interfaces for Kinesis streams and S3. In addition to that all big data processing frameworks supply an SQL interface, examples include Spark, Flink, Hive and Preto. While all of them have different SQL dialects, there's still a common, standardized base syntax, keeping the learning curve low when switching between frameworks. That being said, SQL should not be used as the hammer that fits every nail, it is not a programming language and rather hard to test. For ETL pipelines, usually other languages are a better choice."
Bash,adopt,languages & frameworks,TRUE,"For basic scripting and for glueing together various operational components we have adopted <a href=""https://www.gnu.org/software/bash/"">bash</a> scripts due to their relative simplicity and timelessness."
Scala,adopt,languages & frameworks,TRUE,"<a href=""https://www.scala-lang.org/"">Scala</a> is a JVM based, high-level programming laguage combines object-oriented and functional programming in one concise. We use it to implement some parts of our Data Platform like pipelines or tools."
Spark,adopt,languages & frameworks,TRUE,"<a href=""https://spark.apache.org/"">Apache Spark</a> is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. We use it to build ETL pipelines and to proccess data in scale."
Python 3,adopt,languages & frameworks,TRUE,"<a href=""https://www.python.org/"">Python</a> 3 is now widely accepted in the industry and we have adopted it for all new python components."
TensorFlow,adopt,languages & frameworks,TRUE,"<a href=""https://www.tensorflow.org/"">TensorFlow</a> is a machine learning framework iwith a great ecosystem. We use it regularly in production."
Kotlin,assess,languages & frameworks,TRUE,"<a href=""https://kotlinlang.org/"">Kotlin</a> is a relatively young programming language that has already gained some traction in the industry, for example as the standard language for Android development and by having native support by the Spring Framework. It is based on the Java Virtual Machine which allows for good interoperability with existing Java libraries. The syntax of Kotlin strikes a good balance between Java's verbosity and Scala's conciseness, making it easier to learn than Scala, especially for existing Java developers. Kotlin is being developed by JetBrains, the company behind IntelliJ IDEA, which means that the developer experience in IntelliJ is usually better than with Scala. Spark applications can also be written in Kotlin by using Java SDK, although it is not supported by Zeppelin yet. Therefore, prototyping Spark applications is still easier in Scala."
R,hold,languages & frameworks,TRUE,"<a href=""https://www.r-project.org/"">R</a> is a programming language many data scientists in the industry still use, with emphasis on traditional statistical methods. Similar to RStudio, it is not the choice of development any more at Scout24. "
Python 2,hold,languages & frameworks,TRUE,"With the critical mass finally behind <a href=""https://www.python.org/"">Python</a> 3 and the coming end of support for Python 2, we decided to put all new uses for Python 2 on hold."
Java,hold,languages & frameworks,TRUE,"<a href=""https://go.java"">Java</a> is a programming language that has long been the standard language of ImmobilienScout24. It was also the language of choice of the Data Platform for a long time, with components like the Hadoop REST API, One Scout User DB, and DataWario being written in Java. Java is still a very good general purpose language, however it is also quite verbose, making code usually harder to read and less concise. It also doesn't support new programming paradigms like functional programming as well as other languages like Scala."
Amazon DynamoDB,adopt,platforms,TRUE,"We heavily exploit <a href=""https://aws.amazon.com/dynamodb/"">Amazon DynamoDB</a> for saving state in several components of our Data Platform."
DataDog,adopt,platforms,TRUE,"<a href=""https://www.datadoghq.com/"">Datadog</a> is our central monitoring and operational dashboard solution. All systems metrics are either pushed directly to DataDog via the agent or to CloudWatch which in turn gets pushed to DataDog."
AWS Lambda,adopt,platforms,TRUE,"We use <a href=""https://aws.amazon.com/lambda/"">AWS Lambda</a> functions for various small services, however we have found that a completely serverless approach to data processing can be very expensive and difficult to maintian, with little benefit in scalability. So, while Lambda has an important place in our tech stack, it will not yet replace EC2 (or EMR) based batch processes."
Amazon EMR,adopt,platforms,TRUE,"<a href=""https://aws.amazon.com/emr/""><Amazon EMR</a> is our core Big Data platform. We use it for much of our data preparation and ETL jobs, for our query engines (see Presto entry) and to power our ad hoc Spark clusters (see PAC entry)."
Amazon EC2,adopt,platforms,TRUE,"There are several use cases we are responsible for where using a container service (ECS or Infitity) is not yet possible. In this case, directly launch <a href=""https://aws.amazon.com/ec2/"">Amazon EC2</a> instances. However, we still attempt to dockerize the services running on the EC2 instance."
Amazon S3,adopt,platforms,TRUE,"We have adopted <a href=""https://aws.amazon.com/s3/"">Amazon S3</a> for many purposes such as storage of assets on deployment, storage of raw logs, backups, etc. but most importantly we use S3 for our data lake. All incoming raw and subsequently processed data is stored in S3 buckets."
Amazon CloudWatch,adopt,platforms,TRUE,"All services in our Data Platform push metrics and logs to <a href=""https://aws.amazon.com/cloudwatch/"">Amazon CloudWatch</a>. It is our main destination for investigating issues and observing the behavior of our services. However, we are increasingly relying on DataDog (see its own entry) as all CloudWatch metrics are pushed to DataDog."
Amazon Aurora,adopt,platforms,TRUE,"For cloud-based infrastructure that necessitates using a relational database, we adopt <a href=""https://aws.amazon.com/rds/aurora/"">Amazon Aurora</a> (on RDS) for its superior scalability, features and speed. Currently most of our databases in the cloud are MySQL compliant even though this is not a hard requirement and might change in the future to PostgreSQL compatible Aurora clusters."
Amazon Kinesis Data Firehose,adopt,platforms,TRUE,"We have recently adopted <a href=""https://aws.amazon.com/kinesis/data-firehose/"">Amazon Kinesis Data Firehose</a> as our preferred method of ingesting large amounts of disparate event data from the entire company's services to our data lake. Firehose allows the data producing team to easily push records of events (such as a website click) to the data lake, without worrying about collecting them together and writing to the data lake in a reasonable format."
MicroStrategy,adopt,platforms,TRUE,"<a href=""https://www.microstrategy.com/us"">MicroStrategy</a> is an enterprise business intelligence application, which supports interactive dashboards, scorecards, alerts, ad hoc queries, reports scheduling and distribution. MicroStrategy provides flexible self-service solution for advanced analytics and is used by everybody in the company who wants to analyze and visualize their data."
AWS Data Pipeline,trial,platforms,TRUE,"<a href=""https://aws.amazon.com/datapipeline/"">AWS Data Pipeline</a> is a web service that helps to build the process running between different AWS services. With AWS Data Pipeline, data can be accessed, tranformed and processed at scale. ETL jobs can be easily monitored as well. We use AWS Data Pipelines extensively, however we are not happy with the very low investment from AWS into the service. Although it appears mostly abandoned, there is no replacement that fully matches our needs."
Amazon Kinesis Data Streams,trial,platforms,TRUE,"We currently use <a href=""https://aws.amazon.com/kinesis/data-streams/"">Amazon Kinesis Data Streams</a> for most of the streaming use cases in the data platform and supporting infrastructure. However, we have not yet performed a full investigation to be sure this is the right technology for the foundation of all streaming use cases in the data lake. "
Google Big Query,trial,platforms,TRUE,"<a href=""https://cloud.google.com/bigquery/"">Google Big Query<a> is ideally suited for advanced analysis of our Google Analytics data because there is no need to move the data into our data lake first. However, we leave Big Query in the trial state because the benefits may not overshadow the extra cost in learning and maintainance incurred by having Google Analytics data analysed in a separate environment from the rest of the Data Platform."
Amazon Elasticache,trial,platforms,TRUE,"<a href=""https://aws.amazon.com/elasticache/"">Amazon Elasticache</a> is used by some of our production use cases, however it has not yet emerged as the fully adopted solution for caching in our big data systems."
Presto on EMR,trial,platforms,TRUE,"After using Hive and SparkSQL in production, we have switched to <a href=""https://prestodb.github.io/"">Presto</a> for the main SQL query engine to power all BI use cases in the Data Platform. We are currently trialing it in production to determine if Presto, as shipped with EMR, is mature enough and if its performance is high enough to fully adopt it."
Amazon ECS,trial,platforms,TRUE,"Although our in-house Infinity service provide container services with several advantages, in some cases we rely directly on <a href=""https://aws.amazon.com/ecs/"">Amazon Elastic Container Service</a>. However, there is a good chance that we will move away from ECS to Infinity in the future."
Alation,trial,platforms,TRUE,"<a href=""https://alation.com/"">Alation</a> is the data catalog where everyone in the organization can find the data they need to collaborate. Alation automatically indexes data by source and automatically gathers knowledge about data. Alation uses machine learning to continually improve human understanding. Alation helps to work better together, use data with confidence, improve productivity, and index all data knowledge. Alation currently being used by (DnA) Analysts. After bugs with MicroStrategy connection are fixed, it will be rolled out to broader range of users. "
Amazon Sagemaker,assess,platforms,TRUE,"In line with our strategy of relying on managed services from AWS to lower our maintanence and technical tinkering costs, we are assessing whether <a href=""https://aws.amazon.com/sagemaker/"">Amazon Sagemaker</a> is ready for prime time for our machine learning use cases."
Infinity,assess,platforms,TRUE,"<a href=""https://docs.cloud.scout24.com/products/infinity/"">Infinity</a> is Scout24's internal solution for running applications as Docker containers. The infrastructure for running the containers is managed by the Cloud team, reducing management effort on our side. For example, using Infinity would remove the need for updating AMIs regularly for security reasons. While Infinity is not suitable for workloads running on EMR or in Data Pipeline, it might be usable for the One Scout User DB or other services currently running on EC2 instances managed by our team."
ThoughtSpot,assess,platforms,TRUE,"<a href=""https://www.thoughtspot.com/resources"">ThoughtSpot </a> is business-intelligence analytics search software and a leader in search and AI-driven analytics. It allows for non-technical individuals to conduct a self-service data analysis search. ThoughtSpot includes a machine learning algorithm that presents data suggestions to guide users as input is being typed. It also automatically graphs data sources and joins tables to calculate answers across previously siloed databases. It has a   visualization engine that sorts through possible charts and graphs and presents users with the one best suited to their search. We plan to assess ThoughtSpot because we believe it is a next-generation analytics platform which offers a great business value to our users."
Kylo,assess,platforms,TRUE,"<a href=""https://kylo.io/"">Kylo </a> is Kylo is an open source enterprise-ready data lake management software platform for self-service data ingest and data preparation with integrated metadata management, governance, security and best practices inspired by Think Big’s big data implementation projects. We plan to assess Kylo because we believe it provides an easy self-service solution for data ingestion which will be suitable even for non-technical users, as well as for data preparation with help of visual SQL."
Amazon Redshift,assess,platforms,TRUE,"We are assessing whether an enterprise data warehousing solution (in this case <a href=""https://aws.amazon.com/redshift/"">Amazon Redshift</a>) is a necessary component of our big data infrastructure. It is still not clear to us if we can support all classical BI use cases using only data lake style storage and query solutions."
AWS Glue,assess,platforms,TRUE,"<a href=""https://aws.amazon.com/glue/"">AWS Glue</a> is an exciting new offering from AWS that has the potential to replace some of our non-managed components in the Data Platform (metastore, data pipeline orchestration and notebooks for ad hoc investigations). However, as a new service there is some necessary features missing (like cross account access) and the pricing feels too high for the value provided. Regardless, we are keeping a close eye on the development of AWS Glue and will look to adopt some of its offerings as they improve."
Spikes,adopt,techniques,TRUE,"Spikes are a good way to gather all the ideas from all team members regarding certain technical topics, such as approaches to tackle certain problems, technology to be applied, etc."
Allow for remote collaboration,adopt,techniques,TRUE,"We concously choose to have our meetings set up in a way that they can be joined remotely, be it for the purposes of enabling home-office occasionally, or cross-location collaborations between the BER and MUC offices. This also entails the strong reliance on a variety of other tools (like JiRA and Confluence) to make decisions and work transparent to the entire team."
Pair programming,adopt,techniques,TRUE,"Pair programming is an agile software development technique in which two programmers work together at one workstation. One, the driver, writes code while the other, the observer or navigator, reviews each line of code as it is typed in. The two programmers switch roles frequently. We use this technique daily bases to preduce better code with fewer defects."
Architecture decision records,adopt,techniques,TRUE,"As a means to continuously improve our processes, we adopt retrospectives. We use them to look back at the previous cycle, determine what went well and what we can do better in the future."
Retrospectives,adopt,techniques,TRUE,"We've fully adopted the agile process of regular team retrospectives because they give us a chance to regularly step away from the day to day technical work to focus on improving our internal team processes."
Pull requests with no review,adopt,techniques,TRUE,"Pull Requests as a general feaure are useful to group a series of commits that delivers a particular feature or solves a task. In particular, one can see at a glance what was changed and why (in combination with comments). However, we do not intend to peer-review these changes before merging them to production as we practice pair-programming and thus will naturally fill in for the purposes of a review."
Cleanup Fridays,trial,techniques,TRUE,"Some of our teams are responsible for a large amount of infrastructure that was developed over several years. To address the accumulated code debt (and code rot), we take one Friday per month where all team members can cleanup whatever they like: build processes, upgrade libraries, improve documentation, delete dead code, remove unused cloud resources, etc."
Test driven development,trial,techniques,TRUE,"When a bug shows up, it could be adjusted to be a test case. Then the production code will be modified in order to get this test pass."
Monitor system health,trial,techniques,TRUE,"As we are responsible for a far amount of cloud infrastructure, we need good observability practices to quickly become aware of problems. While we fully agree it is necessary to monitor system health, we have not yet agreed on a standard framework or process for doing so. Therefore, we left this practice in the Trial state."
End-to-end development,trial,techniques,TRUE,"As we move toward thinking of the data platform as a stand alone product, we are trialing standard Product Management techniques, such as end-to-end development. That is, we do not develop new features in layers, but rather take a small feature subset and built out a full stack (data, backend, plus UI) for that feature. This allows us to integrate early and often, which catches major architectural issues early, and it allows us to gather feedback on our approach as early as possible."
Causal inference with A/B testing,trial,techniques,TRUE,"Causal inference is the process of drawing conclusion about a casual connection based on the conditions of the occurrence of an effect. A/B testing is a common practice of causal inference adopted by the tech industry. There are plenty A/B testing and analysis tools readily available for analysts and PMs, yet very few covers the rigor and scientific basis."
Platform approach,trial,techniques,TRUE,"Scout24 has launched a large scale trial of what we call they 'platform approach'. This means that teams that provide central services should consider their job to be building a platform upon which the rest of the company can build their products. We has a Cloud Platform that builds on AWS application services to make it simple for product teams to build and deploy their application, and we offer a Data Platform that offers a simplified set of infrastructure and tools to manage the companies data. We believe that the platform approach will allow us to scale much faster in our cloud adoption and improvements to data-driven decision making."
Microtools,trial,techniques,TRUE,"The term ""Microtools"" is based on <a href=""https://microservices.io/"">Microservices</a>. A microservice is a system that has only one specific task. To enable a complete usecase, these services would then interact via well-defined interfaces. In our data platform, we usually don't provide many ""services"" but instead focus on tools that make working with data easier for the users. Putting the analogy from microservices to work with microtools means, that our tools do one job very well. For example the event snapshotter focuses on the snapshotting task, and does not provide the possibility to aggregate metrics, convert data or create tables ontop of the data. For these tasks we provide other tools. This improves usability, because each tool doesn't have many configuration settings, and testability, because there are less possible conditions that can apply. (see also the <a href=""https://en.wikipedia.org/wiki/Single_responsibility_principle"">Single responsibility principle""</a>)"
Kanban with regular planning intervals,trial,techniques,TRUE,"We tend to use Kanban-style agile software development with regular planning sessions. Over the years, we’ve found that this combination of Scrum and Kanban is the right mix for us. However, we leave this technique in Trial because we are continually tweaking how we work."
Versioning ML models and data,assess,techniques,TRUE,"Just like it is standard practise to track version of source code, we are assessing versioning machine learning models and training data. We expect the same benefits of reproducability and knowledge sharing will come from this practise."
Hack days,assess,techniques,TRUE,"Innovation is a necessary component of any software company and we are assessing whether regular team-centric hack days are a good technique for occasionally stepping away from our day-to-day work to looks at problems from a new perspective."
Game days,assess,techniques,TRUE,"Game days are a disaster recover technique pioneered by AWS. During a game day, a team attempts to reproduce their cloud infrastructure in a test environment. The goal is to practice disaster recovery but also to improve our infrastructure as code and CI/CD practises."
Gradual rollouts to production,assess,techniques,TRUE,"In order to minimize the user impact on rolling out new changes to our infrastructure, we want to assess techniques that would do this rollout gradually such that we can tightly monitor system behavior and can quickly react in case the infrastructure does not behave as intended."
PRFAQ process,assess,techniques,TRUE,"We are assessing the 'write the press release first' approach to new product/feature development. We refer to this as the PRFAQ process because a proposal for a new product/feature include a Press Release and a FAQ document written as if the product has already been built. This practise is inspired by the process using with Amazon and many other tech companies. Our first tests of this approach have indicated it has promise, especially in gaining clarity in purpose and alignment across related teams."
Survey user satisfaction,assess,techniques,TRUE,"As an internal platform and services group, we don't immediately think of our users in the same ways as external product developers do. However, we believe this is a mistake and we are taking steps to improve the feedback we receive from our ""customers""."
Code reviews,hold,techniques,TRUE,"We put code-reviews on hold as we see the benefit of always having multiple people work on tasks together (see pair programming). This fosters natural knowledge sharing and does away with the necessity to have formal reviews.